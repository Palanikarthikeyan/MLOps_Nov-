{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25280798-50de-43db-89f2-4def5afdacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-rdd-transformations/\n",
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "Recap\n",
    "======\n",
    "PySpark \n",
    "- follows Master-Slave design\n",
    "- key components - python+java+scala \n",
    "- pyspark - use lib - Py4j\n",
    "- Py4j - bridge between Python and Core spark components - Scala - run JVM\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "SparkSession.builder.appName('userdefinedAppName').getOnCreate() //sparkContext\n",
    "sparkContext.createDataFrame()\n",
    "\n",
    "DataFrame\n",
    "RDD \n",
    "Transformation - filter select map ....\n",
    "action - execution -> show() count()\n",
    "\n",
    "CPU-2-core\n",
    "4GB RAM\n",
    "    |\n",
    "    open(\"data.csv\",\"r\")\n",
    "           |\n",
    "          5GB size\n",
    "\n",
    "    pandas - pd.read_csv('data.csv')\n",
    "                          |->5GB\n",
    "    data.csv\n",
    "      |<---- live data -   registrationForm\n",
    "                            --<---\n",
    "      <--- append to file\n",
    "================================================================\n",
    "app       event_records/\n",
    "   |---->   10_.log       <--------------- load this data\n",
    "   |---->   10.05_log\n",
    "               ...\n",
    "================================================================\n",
    "\n",
    "1. Initialize spark session object\n",
    "2. read \n",
    "3. transformation \n",
    "4. action\n",
    "5. stop session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221d22bb-cd0b-4e6f-a04c-7f707c12f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').getOrCreate()\n",
    "\n",
    "data = [(\"pA\",101),(\"pB\",102),(\"pC\",103)]\n",
    "columns = [\"pName\",\"pID\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "#f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1abdec-2c43-4283-b335-23dd4e7e0ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://paka:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ea77631450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d38c93c-898b-4c6b-8469-c86895c315f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ac8e1-7691-4083-b7dc-a08c4a984239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('prod.csv',header=True) = from local node\n",
    "\n",
    "df2 = spark.read.csv('hdfs://namenode:9000/prod.csv',header=True) = from HDFS\n",
    "\n",
    "df3 = spark.read.csv(\"s3a://bucket/data.csv\") = from Cloud object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395f9ed-6f35-4d49-9e67-045ff4bd257c",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>\n",
    ">>> from pyspark.sql import SparkSession\n",
    ">>>\n",
    ">>> spark_session_obj = SparkSession.builder.appName(\"rdd-demo\").getOrCreate()\n",
    "25/11/05 06:22:12 WARN Utils: Your hostname, paka resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
    "25/11/05 06:22:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "25/11/05 06:22:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    ">>>\n",
    ">>> sc = spark_session_obj.sparkContext\n",
    ">>>\n",
    ">>> L = [1,2,3,4,5]\n",
    ">>> sc.parallelize(L)\n",
    "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n",
    ">>>\n",
    ">>> rdd_obj = sc.parallelize(L)\n",
    ">>> rdd_obj.map(lambda a:a*5)\n",
    "PythonRDD[2] at RDD at PythonRDD.scala:53\n",
    ">>>\n",
    ">>> rdd1 = rdd_obj.map(lambda a:a*5)\n",
    ">>>\n",
    ">>> rdd2 = rdd_obj.filter(lambda a: a>3)\n",
    ">>>\n",
    ">>> rdd1.collect()\n",
    "[5, 10, 15, 20, 25]\n",
    ">>> rdd2.collect()\n",
    "[4, 5]\n",
    ">>>\n",
    ">>> rdd3 = rdd1.filter(lambda a:a>10)\n",
    ">>> rdd3.collect()\n",
    "[15, 20, 25]\n",
    ">>>\n",
    ">>> file_rdd = sc.textFile(\"prod.csv\")\n",
    ">>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aea1c0-3afb-43b3-81b8-e7a25b4532cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "1. initialize sparksession object\n",
    "2. initialize spark context from session object\n",
    "3. using spark_context object read a any text file(.log .csv .txt..) # use sc.textFile('filename')\n",
    "4. using collect() - display file contents\n",
    "----\n",
    "5. use flatMap() - split each line into multiple items(words)\n",
    "   display splitted results \n",
    "----\n",
    "6. use map() - convert each word into tuple\n",
    "   display map results\n",
    "---\n",
    "7. use reduceByKey() - group all values by their key(each word)\n",
    "   then it reduces(aggregates) values for each key using reduceByKey() function.\n",
    "--------------\n",
    "\n",
    "For Example - input data file is data.log\n",
    "(pyspark-env) student@paka:~$ cat >data.log\n",
    "project details\n",
    "list of loaded data model\n",
    "linear data set value\n",
    "list of product details\n",
    "data set value\n",
    "(pyspark-env) student@paka:~$\n",
    "\n",
    "========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce8399-f424-4d79-97f5-f0d284fd9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test1\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.textFile('data.log')\n",
    "print(rdd.collect())\n",
    "print('') # empty line\n",
    "#####################################\n",
    "rdd2 = rdd.flatMap(lambda a:a.split(\" \"))\n",
    "print(rdd2.collect())\n",
    "print('') # empty line\n",
    "#####################################\n",
    "rdd3 = rdd2.map(lambda a:(a,1))\n",
    "print(rdd3.collect())\n",
    "print('') # empty line\n",
    "####################################\n",
    "print(rdd3.reduceByKey(lambda a,b:a+b).collect())\n",
    "################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd79cbf-351a-406c-9004-d5e22a133d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "[10,20,30,40]\n",
    "\n",
    "[10,20]  - partition1\n",
    "[30,40]  - partition2\n",
    "-----------------------//RDD - split the data into smaller chunk(partition)\n",
    "\n",
    "number of partitions = number of cpu core\n",
    "\n",
    "\n",
    "[10,20]  - partition1 -- spark sends each partition to different executor (jvm)\n",
    "[30,40]  - partition2 -- spark sends each partition to different executor (jvm) \n",
    "|\n",
    "transformation ->action\n",
    "---------------------------------\n",
    "\n",
    "userInput - inputlist/collection/dataset \n",
    "|\n",
    "DriverProgram\n",
    "|\n",
    "SparkContext\n",
    "|\n",
    "+---------------------+\n",
    "| Partition1:[10,20]  ----------> | executor - jvm1 | + 100 =>[110,120]\n",
    "+---------------------+\n",
    "| Partition2:[30,40]  ----------> | executor - jvm2 | + 100 =>[130,140]\n",
    "+---------------------+\n",
    "    |\n",
    "    Result -- collected back ==> [110,120,130,140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741b07f-f7f1-4666-87e2-104f7fa512e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Batch Processing\n",
    "=====================\n",
    "Process - finite data(complete dataset) - stored in DB/file - fetch the complete data do \n",
    "                                                              process at once.\n",
    "\n",
    "df = pd.read_csv('emp.csv')\n",
    "df.shape ->(10,5) Ex: executed on time 10:05 - df.shape\n",
    "df.shape ->(10,5) Ex: executed on time 10:10 - df.shape\n",
    "# vi emp.csv\n",
    "# added two more lines \n",
    "# :wq \n",
    "# ------------- time on 10:15 \n",
    "df.shape ->(10,5) Ex: executed on time 10:20 - df.shape \n",
    "\n",
    "Vs\n",
    "2. Stream Processing\n",
    "=====================\n",
    "Process - infinite data (or) real-time data\n",
    "\n",
    "\n",
    "    +----------------------------------+\n",
    "    | Streaming Source  <== Socket,File,Kafka etc.,\n",
    "    |                       ---(input source)---\n",
    "    +-----------|-----------------------+\n",
    "    | SparkStructured Streaming         |\n",
    "    |    (Transformation)               |\n",
    "    +-----------|-----------------------+\n",
    "    |   Streming Sink ---->Console,HDFS,Delta Lake,Kafa etc.,\n",
    "    |                       (Output sink)\n",
    "    +-----------------------------------+\n",
    "\n",
    "    Stream output modes\n",
    "    |->append mode  --- adds only new records/row to result table - not using aggregate method\n",
    "    |->update mode --- increment value/recent value  - aggregate\n",
    "    |->complete mode -- full value - aggregate\n",
    "\n",
    "\n",
    "1st - spark session object\n",
    "2nd - spark session object interface to inputmode  ->df\n",
    "3rd - df.do_Transformation ->result\n",
    "4th - result ->Sink_to_output\n",
    "\n",
    "\n",
    "DataStream                        UnboundedTable\n",
    "--------------                    ------------------\n",
    "  ----------------->                  ...\n",
    "                                   ----------------\n",
    "\n",
    " ------------->|Sparksteam | -->[][][][] ---->|Spark core |-->outputSinks\n",
    "                              inputbatches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16485a8f-2ba6-4425-943a-d9ccaf84cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101,\"pA\",1000,\"USA\"),\n",
    "    (102,\"pB\",2000,\"Canada\"),\n",
    "    (103,\"pC\",3000,\"India\"),\n",
    "    (104,\"pD\",4000,\"USA\"),\n",
    "    (105,\"pE\",3400,\"India\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf0207-33a3-4f85-96ca-461ffe516532",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from pyspark.sql import SparkSession\n",
    ">>> from pyspark.sql.functions import col\n",
    ">>> data = [(\"pA\",101,1000),(\"pB\",102,2000),(\"pC\",103,3000)]\n",
    ">>>\n",
    ">>> columns = [\"pname\",\"pid\",\"pcost\"]\n",
    ">>>\n",
    ">>> df = spark_obj.createDataFrame(data,columns)\n",
    ">>> df.show()\n",
    "+-----+---+-----+\n",
    "|pname|pid|pcost|\n",
    "+-----+---+-----+\n",
    "|   pA|101| 1000|\n",
    "|   pB|102| 2000|\n",
    "|   pC|103| 3000|\n",
    "+-----+---+-----+\n",
    ">>> new_df = df.withColumn(\"Tax\",col(\"pcost\") * 0.12)\n",
    ">>>\n",
    ">>> new_df.show()\n",
    "+-----+---+-----+-----+\n",
    "|pname|pid|pcost|  Tax|\n",
    "+-----+---+-----+-----+\n",
    "|   pA|101| 1000|120.0|\n",
    "|   pB|102| 2000|240.0|\n",
    "|   pC|103| 3000|360.0|\n",
    "+-----+---+-----+-----+\n",
    "\n",
    ">>> # df['tax']=df['pcost']*0.12 # in pandas\n",
    ">>> # ----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff376b2-0b9b-4a82-80bb-24e38b8a3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Create a dataframe\n",
    "2. use the above dataframe ->select only the product and price columns - cost above 3500\n",
    "    Hint: select() filter() where()\n",
    "3. Add new column -> withColumn() - transformation\n",
    "                     df.withColumn(\"<ColumnName>\",ColValue)\n",
    "|\n",
    "4. Aggregation and Grouping \n",
    "    -> df.groupBy(\"country\").sum() / avg()\n",
    "5. display\n",
    "\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a1361-355a-4651-9b24-7c4089705ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark_obj = SparkSession.builder.appName(\"Activity-1\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (101,\"pA\",1000,\"USA\"),\n",
    "    (102,\"pB\",2000,\"Canada\"),\n",
    "    (103,\"pC\",3000,\"India\"),\n",
    "    (104,\"pD\",4000,\"USA\"),\n",
    "    (105,\"pE\",3400,\"India\")\n",
    "]\n",
    "columns = [\"productID\",\"product\",\"cost\",\"country\"]\n",
    "\n",
    "prod_df = spark_obj.createDataFrame(data,columns)\n",
    "print(prod_df.show())\n",
    "\n",
    "r1 = prod_df.filter((col(\"country\") == \"India\") & (col(\"cost\") >2560))\n",
    "print(r1.show())\n",
    "\n",
    "r2 = r1.select(\"product\",\"cost\")\n",
    "print(r2.show())\n",
    "\n",
    "# To add new column \n",
    "final_df = prod_df.withColumn(\"Tax\",col(\"cost\") * 0.18)\n",
    "final_df.show()\n",
    "\n",
    "# Aggregation \n",
    "r3 = prod_df.groupBy(\"country\").sum(\"cost\")\n",
    "r3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45334090-68a5-4138-bf20-d6885b4d2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,desc\n",
    "\n",
    "spark_obj = SparkSession.builder.appName(\"Activity-1\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (101,\"pA\",1000,\"USA\"),\n",
    "    (102,\"pB\",2000,\"Canada\"),\n",
    "    (103,\"pC\",3000,\"India\"),\n",
    "    (104,\"pD\",4000,\"USA\"),\n",
    "    (105,\"pE\",3400,\"India\")\n",
    "]\n",
    "columns = [\"productID\",\"product\",\"cost\",\"country\"]\n",
    "\n",
    "prod_df = spark_obj.createDataFrame(data,columns)\n",
    "prod_df.show()\n",
    "\n",
    "r1 = prod_df.filter((col(\"country\") == \"India\") & (col(\"cost\") >2560))\n",
    "r1.show()\n",
    "\n",
    "r2 = r1.select(\"product\",\"cost\")\n",
    "r2.show()\n",
    "# To add new column\n",
    "final_df = prod_df.withColumn(\"Tax\",col(\"cost\") * 0.18)\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "# Aggregation\n",
    "r3 = prod_df.groupBy(\"country\").sum(\"cost\")\n",
    "r3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f6ec0-802a-41cc-a7be-830736acc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    +----------------------------------+\n",
    "    | Streaming Source  <== Socket,File,Kafka etc.,  - readStream.format().<config>.load()\n",
    "    |                       ---(input source)---\n",
    "    +-----------|-----------------------+\n",
    "    | SparkStructured Streaming         |\n",
    "    |    (Transformation)               |\n",
    "    +-----------|-----------------------+\n",
    "    |   Streming Sink ---->Console,HDFS,Delta Lake,Kafa etc., - writeStream.format().start()\n",
    "    |                       (Output sink)\n",
    "    +-----------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50d6bb-b9e1-4425-aed8-92a89edf8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stream \n",
    "from pyspark.sql import SparkSession\n",
    "spark_obj = SparkSession.builder.appName(\"socketdemo\").master(\"local[*]\").getOrCreate()\n",
    "stream_df = spark_obj.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",\"1100\").load()\n",
    "\n",
    "print(stream_df.isStreaming) # ->bool(True,False)\n",
    "print(stream_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204dc81-00ad-4ac3-8557-81cf88464ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input stream source is file => input_directory/ \n",
    "                                              |->newfile1 |->newfile2 ...\n",
    "define the schema  =>\n",
    "\n",
    "defined data type -> from pyspark.sql.types import StructType\n",
    "                                                      |->StringType,IntegerType...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4547b7-3cac-4219-a3c6-3a1598a1d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "readStream.format(\"socket\")  .. ->df\n",
    "df.writeStream.outputMode(\"append\").format(\"csv\")......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf503e8-0137-43bb-a722-4e5285149f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WebApp -- producer\n",
    " |\n",
    "Kafka\n",
    "|\n",
    "Analytics service - consumer \n",
    "\n",
    "\n",
    "Kafka is running -(on in docker)\n",
    "|\n",
    "kafka => kafa-topic.sh --create --topic transactions --bootstarp-server localhost:9092 {Enter}\n",
    "kafa-console-producer.sh --topic transaction --bootstrap-server localhost:9092 {enter}\n",
    "  data - json \n",
    "      {\"pname\":\"pA\",\"pid\":1000,...}\n",
    "--------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"filestream\").getOrCreate()\n",
    "\n",
    "schema_obj = StructType([StructField(\"pname\",StringType(),True),StructField(\"pid\",IntegerType(),True)])\n",
    "\n",
    "kafaka_stream = spark.readStream.format(\"kafka\").option(\"kafaka.bootstarp.servers\",\"localhost:9092\")\\\n",
    ".option(..).load()\n",
    "\n",
    "..\n",
    "kafaka_stream.writeStream.outputMode(\"complete\").format(\"console\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787801bb-aae6-46b4-99ed-7df5b810dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').getOrCreate()\n",
    "\n",
    "data = [(\"pA\",101),(\"pB\",102),(\"pC\",103)]\n",
    "columns = [\"pName\",\"pID\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c202ed-3b75-4254-b3e1-c59373335482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').getOrCreate()\n",
    "\n",
    "data = [(\"pA\",101),(\"pB\",102),(\"pC\",103)]\n",
    "columns = [\"pName\",\"pID\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4efb2-f29c-41c8-8913-18fa444037ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').getOrCreate()\n",
    "\n",
    "data = [(\"pA\",101),(\"pB\",102),(\"pC\",103)]\n",
    "columns = [\"pName\",\"pID\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data,columns)\n",
    "\n",
    "df2 = spark.read.csv('prod.csv',header=True)\n",
    "df1.printSchema()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e113d9-1f75-472a-9467-11a35b2d6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.csv('prod.csv',header=True)\n",
    "df.select(\"productName\").show()\n",
    "df.filter(df.productCost >1250).show()\n",
    "df.groupBy(\"productID\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01fe58-cf6b-49d8-8f81-fe17ad108c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.csv('prod.csv',header=True)\n",
    "df.select(\"productName\").show()\n",
    "df.filter(df.productCost >1250).show()\n",
    "df.groupBy(\"productID\").count().show()\n",
    "\n",
    "df.createOrReplaceTempView(\"prod\")\n",
    "result = spark.sql(\"select *from prod where productCost >3000\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf247f3f-b98e-4f2e-aeee-39a5433feb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## rdd examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cacef3-fdf4-4b2d-a4de-57ca35d372ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([10,20,30,40,50])\n",
    "print(\"partition count:\",rdd.getNumPartitions())\n",
    "rdd = sc.parallelize([10,20,30,40,50],4)\n",
    "print(\"partition count:\",rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9410dd-b071-4e3b-8ac6-14865c4366d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# create an RDD with 2 partitions\n",
    "rdd = sc.parallelize([10,20,30,40,50,60],2)\n",
    "\n",
    "# To get partitions count\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "rdd1 = rdd.map(lambda a:a+100)\n",
    "########################### immutable  - not changable - initialize to another rdd\n",
    "\n",
    "print(rdd1.collect()) # collect and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e9874-a47f-4a7a-b0f1-0c8481666e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# create an RDD with 2 partitions\n",
    "rdd = sc.parallelize([10,20,30,40,50,60],2)\n",
    "\n",
    "mapped_obj = rdd.map(lambda a:[a,a+100])\n",
    "print(mapped_obj.collect())\n",
    "print('') # empty line\n",
    "\n",
    "rdd = sc.parallelize([10,20,30,40,50,60],2)\n",
    "flat_mapped_obj = rdd.flatMap(lambda a:[a,a+100])\n",
    "print(flat_mapped_obj.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc1b8f-69a2-46a2-befd-6fad36d0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([4,5,6])\n",
    "rdd_result = rdd1.union(rdd2)\n",
    "print(rdd_result.collect())\n",
    "print(\"\")\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([3,5,6])\n",
    "rdd_result = rdd1.union(rdd2)\n",
    "print(rdd_result.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fdc10-0b37-4c08-8417-8d52b6574662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([(\"row1\",10),(\"row2\",20),(\"row3\",30),(\"row1\",40),(\"row2\",50)])\n",
    "print(rdd.reduceByKey(lambda a,b:a+b).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c8436-48e9-4b3b-9ecb-c61a7b3ab116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark with Delta Lake\n",
    "# --------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaLakedemo1\").config(\"spark.sql.extension\",\"io.delta.sql.DeltaSparkSessionException\").config(\"spark.sql.catalog.spark_catlog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n",
    "\n",
    "data = [(\"pA\",101),(\"pB\",102),(\"pC\",103)]\n",
    "df = spark.createDataFrame(data,[\"pname\",\"pid\"])\n",
    "\n",
    "df.write.format(\"delta\").save(\"/tmp/delta-table\")\n",
    "\n",
    "# Read-Delta table\n",
    "#spark.read.format(\"delta\").load(\"./delta-table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a49845-5429-42d9-919f-41865c9ef742",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pyspark-env) student@paka:~$ cat rdd_activity1.py\n",
    "'''\n",
    "# Task\n",
    "1. initialize sparksession object\n",
    "2. initialize spark context from session object\n",
    "3. using spark_context object read a any text file(.log .csv .txt..) # use sc.textFile('filename')\n",
    "4. using collect() - display file contents\n",
    "----\n",
    "5. use flatMap() - split each line into multiple items(words)\n",
    "   display splitted results\n",
    "----\n",
    "6. use map() - convert each word into tuple\n",
    "   display map results\n",
    "---\n",
    "7. use reduceByKey() - group all values by their key(each word)\n",
    "   then it reduces(aggregates) values for each key using reduceByKey() function.\n",
    "--------------\n",
    "\n",
    "For Example - input data file is data.log\n",
    "(pyspark-env) student@paka:~$ cat >data.log\n",
    "project details\n",
    "list of loaded data model\n",
    "linear data set value\n",
    "list of product details\n",
    "data set value\n",
    "(pyspark-env) student@paka:~$\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test1\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.textFile('data.log')\n",
    "print(rdd.collect())\n",
    "print('') # empty line\n",
    "#####################################\n",
    "rdd2 = rdd.flatMap(lambda a:a.split(\" \"))\n",
    "print(rdd2.collect())\n",
    "print('') # empty line\n",
    "#####################################\n",
    "rdd3 = rdd2.map(lambda a:(a,1))\n",
    "print(rdd3.collect())\n",
    "print('') # empty line\n",
    "####################################\n",
    "print(rdd3.reduceByKey(lambda a,b:a+b).collect())\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82dd180-dbcc-4529-9c5f-e6b762a7edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### streaming examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636282a-930b-418c-9823-22beea5afb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# socket stream\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "print(df.isStreaming) # ->bool(True/False)\n",
    "print(df.printSchema())\n",
    "write_query = df.writeStream.format(\"console\").start()\n",
    "write_query.awaitTermination() # keep on running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edb620-39b7-4531-90b3-712ccc52a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "print(df.isStreaming) # ->bool(True/False)\n",
    "print(df.printSchema())\n",
    "###\n",
    "df_result = df.select(explode(split(\"value\",\" \")).alias(\"word\"))\n",
    "\n",
    "word_count = df_result.groupBy(\"word\").count()\n",
    "###\n",
    "#\n",
    "# write_query = word_count.writeStream.format(\"console\").start()\n",
    "# Error - default outputMode is append\n",
    "\n",
    "write_query = word_count.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "write_query.awaitTermination() # keep on running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88515d49-1b6e-4ce7-95c9-7b82ea22550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filestream examples\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "print(df.isStreaming) # ->bool(True/False)\n",
    "print(df.printSchema())\n",
    "###\n",
    "df_result = df.select(explode(split(\"value\",\" \")).alias(\"word\"))\n",
    "\n",
    "word_count = df_result.groupBy(\"word\").count()\n",
    "###\n",
    "#\n",
    "# write_query = word_count.writeStream.format(\"console\").start()\n",
    "# Error - default outputMode is append\n",
    "\n",
    "write_query = word_count.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "write_query.awaitTermination() # keep on running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6e150-9101-4f81-9d9c-67aeab6d40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"filestream\").getOrCreate()\n",
    "\n",
    "schema_obj = StructType([StructField(\"pname\",StringType(),True),StructField(\"pid\",IntegerType(),True)])\n",
    "\n",
    "file_stream = spark.readStream.option(\"header\",\"true\").schema(schema_obj).csv(\"input_dir/\")\n",
    "\n",
    "r = file_stream.filter(file_stream.pid >100).groupBy(\"pname\").count()\n",
    "\n",
    "r.writeStream.outputMode(\"update\").format(\"console\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b5f57-b4a1-492d-a55b-3cb1f6254628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"filestream\").getOrCreate()\n",
    "\n",
    "schema_obj = StructType([StructField(\"pname\",StringType(),True),StructField(\"pid\",IntegerType(),True)])\n",
    "\n",
    "file_stream = spark.readStream.option(\"header\",\"true\").schema(schema_obj).csv(\"input_dir/\")\n",
    "\n",
    "#r = file_stream.filter(file_stream.pid >100).groupBy(\"pname\").count()\n",
    "\n",
    "file_stream.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\n",
    "#r.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\n",
    "#r.writeStream.format(\"console\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf9f7a-d1af-4605-aeb6-03ed5628c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"jsonstream\").getOrCreate()\n",
    "\n",
    "schema_obj = StructType([StructField(\"pname\",StringType(),True),StructField(\"pid\",IntegerType(),True)])\n",
    "\n",
    "json_stream = spark.readStream.schema(schema_obj).json(\"json_dir/\")\n",
    "\n",
    "#r = file_stream.filter(file_stream.pid >100).groupBy(\"pname\").count()\n",
    "\n",
    "json_stream.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\n",
    "#r.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()\n",
    "#r.writeStream.format(\"console\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed325bb5-6309-4807-8397-bb254a8fede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from socket stream -->filestream - examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c8c7f-8fab-4e26-914e-6b5415829032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "\n",
    "df.writeStream.outputMode(\"append\").format(\"csv\").option(\"path\",\"output_dir/\").option(\"checkpointLocation\",\"checkpoints/csv_stream_chpt\").option(\"header\",\"true\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826189c6-2e00-46c9-93f3-a86ca26c6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "\n",
    "df_result = df.select(explode(split(\"value\",\" \")).alias(\"word\"))\n",
    "\n",
    "wc = df_result.groupBy(\"word\").count()\n",
    "wc.writeStream.outputMode(\"update\").format(\"csv\").option(\"path\",\"output_dir/\").option(\"checkpointLocation\",\"checkpoints/csv_stream_chpt\").option(\"header\",\"true\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be379f79-9e2c-4083-9274-bb2f1200dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef8827-634e-41f3-a528-86bb237b7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airflow \n",
    "========\n",
    " |->open-source - schedule and monitor workflows (data pipelines)\n",
    "                     |->daily,hourly,custom schedule\n",
    " |-> On linux -> cronjob - - jobschedule \n",
    "\n",
    " |-> airflow ->programming way job schedule and monitor workflow - webUI\n",
    "                  |->python \n",
    "  code sequence ->their dependencies ->schedules -> webUI\n",
    "\n",
    " |->Directed Acyclic Graph (DAG)\n",
    "    ============================\n",
    "       [task1] -->[task2] -->[task3] --->[task4]  # there is no reverse ; no looping\n",
    "\n",
    "    DAG - descibes how to run the workflow\n",
    "\n",
    " |-> task - code(or)unit \n",
    "\n",
    " |-> operators - pre-defined templates for tasks \n",
    "         |-> BashOperator - executes a bash command/script file\n",
    "         |-> PythonOperator - (@task decoratror) - function\n",
    "         |-> SqlOperator - Execute SQL command \n",
    "\n",
    " |-> executor/workers - instance \n",
    "\n",
    " |-> webUI \n",
    "\n",
    " |->services\n",
    "        |->scheduler \n",
    "        |->webserver \n",
    "\n",
    "---------------------------------------------------\n",
    "    airflow/\n",
    "            |->logs/\n",
    "            |->data/\n",
    "            |->dags/ <==\n",
    "                 |<-- dagscript.py\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "Create airflow login \n",
    "|\n",
    "airflow users create --username <admin> --firstname Admin --lastname user --role admin\n",
    "             --email admin@example.com --password <password>{enter}\n",
    "|\n",
    "             Login:\n",
    "             password:\n",
    "------------------------------------------------------------------------\n",
    "# Start webserver => airflow webserver --port 8080 {Enter} \n",
    "\n",
    "# Start scheduler -> airflow scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18567891-04c9-4fea-bf9d-d2c87b8f2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "student@paka:~$ python3 -m venv lab2\n",
    "student@paka:~$ source lab2/bin/activate\n",
    "(lab2) student@paka:~$\n",
    "(lab2) student@paka:~$ export AIRFLOW_VERSION=2.10.1\n",
    "(lab2) student@paka:~$ export PYTHON_VERSION=\"$(python --version|cut -d\" \" -f2|cut -d\".\" -f 1,2)\"\n",
    "(lab2) student@paka:~$ export CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
    "(lab2) student@paka:~$\n",
    "(lab2) student@paka:~$ pip install \"apache-airflow == ${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n",
    "Collecting apache-airflow==2.10.1\n",
    "...\n",
    "(lab2) student@paka:~$ airflow version\n",
    "2.10.1\n",
    "(lab2) student@paka:~$\n",
    "(lab2) student@paka:~$ ls -t\n",
    "airflow <=== \n",
    "\n",
    "\n",
    "(lab2) student@paka:~$ ls airflow\n",
    "logs\n",
    "(lab2) student@paka:~$ airflow db init\n",
    "/home/student/lab2/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py:55 DeprecationWarning: `db init` is deprecated.  Use `db migrate` instead to migrate the db and/or airflow connections create-default-connections to create the default connections\n",
    "DB: sqlite:////home/student/airflow/airflow.db\n",
    "[2025-11-05T12:17:08.492+0000] {migration.py:215} INFO - Context impl SQLiteImpl.\n",
    "[2025-11-05T12:17:08.494+0000] {migration.py:218} INFO - Will assume non-transactional DDL.\n",
    "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
    "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
    "INFO  [alembic.runtime.migration] Running stamp_revision  -> 22ed7efa9da2\n",
    "WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.\n",
    "Initialization done\n",
    "(lab2) student@paka:~$ ls airflow\n",
    "airflow.cfg  airflow.db  logs\n",
    "--------------------------------//this is no dags directory \n",
    "\n",
    "(lab2) student@paka:~$ ls airflow\n",
    "airflow.cfg  airflow.db  logs\n",
    "(lab2) student@paka:~$ mkdir -p ~/airflow/dags # create new dags directory\n",
    "(lab2) student@paka:~$ ls airflow\n",
    "airflow.cfg  airflow.db  dags  logs\n",
    "(lab2) student@paka:~$   ====\n",
    "\n",
    "airflow users create --username admin --firstname stduent --lastname user \n",
    "--role Admin --email test@example.com --password admin\n",
    "\n",
    "## To start webserver => airflow webserver -p 8080\n",
    "\n",
    "dag_script.py <== scheduler script\n",
    "|\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "with DAG() as obj:\n",
    "    dag_id=<userdefined_ dagName>\n",
    "    start_date=datetime(start up date) YYYY,MM,DD\n",
    "    schedule_interval='@daily' 5minutes '@weekly' \n",
    "    catchup=<bool>\n",
    "             |->True => 2025,11,05 - schedule date\n",
    "                            |\n",
    "                       start project on 17th Nov\n",
    "    PythonOperator(task_id=<>,python_callable=<functionName>)\n",
    "     <or>\n",
    "    BashOperator()\n",
    "\n",
    "----------------------------------------------// ~/airflow/dags/dag_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c82b85-4729-4f30-af3a-0505b4a8f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lab2) student@paka:~$ mkdir airflow-activity\n",
    "(lab2) student@paka:~$ cd airflow-activity/\n",
    "(lab2) student@paka:~/airflow-activity$\n",
    "(lab2) student@paka:~/airflow-activity$ vi etl_dag.py\n",
    "(lab2) student@paka:~/airflow-activity$ vi mydb.py\n",
    "(lab2) student@paka:~/airflow-activity$ vi pETL.py\n",
    "(lab2) student@paka:~/airflow-activity$ python mydb.py\n",
    "(lab2) student@paka:~/airflow-activity$ ls -t\n",
    "test1.db  pETL.py  mydb.py  etl_dag.py\n",
    "(lab2) student@paka:~/airflow-activity$ python pETL.py\n",
    "Data written to /home/student/extract/etl_output_20251105124821.csv\n",
    "(lab2) student@paka:~/airflow-activity$ pwd\n",
    "/home/student/airflow-activity\n",
    "(lab2) student@paka:~/airflow-activity$ vi wrapper_script.sh\n",
    "(lab2) student@paka:~/airflow-activity$ cat wrapper_script.sh\n",
    "python /home/student/airflow-activity pETL.py\n",
    "(lab2) student@paka:~/airflow-activity$ chmod a+x wrapper_script.sh\n",
    "(lab2) student@paka:~/airflow-activity$ ls\n",
    "etl_dag.py  mydb.py  pETL.py  test1.db  wrapper_script.sh\n",
    "(lab2) student@paka:~/airflow-activity$ cp etl_dag.py ~/airflow/dags/\n",
    "(lab2) student@paka:~/airflow-activity$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e6f16-a4cb-4594-8453-e0964e917c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## end of the day3 ############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
